{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Core utilities for Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from dreamai_dl.utils import *\n",
    "from dreamai_dl.imports import *\n",
    "from dreamai_dl.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SLModel(L.LightningModule):\n",
    "    \"LightningModule for single label classification.\"\n",
    "    \n",
    "    def __init__(self, model, learning_rate, cosine_t_max, in_chans=3, seed=42):\n",
    "        super().__init__()\n",
    "        L.seed_everything(seed)\n",
    "        store_attr(\"model,learning_rate,cosine_t_max\")\n",
    "        self.example_input_array = torch.Tensor(1, in_chans, 224, 224)\n",
    "        self.num_classes = model.num_classes\n",
    "        self.save_hyperparameters(ignore=[\"model\"])\n",
    "        self.train_acc = TM.Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
    "        self.val_acc = TM.Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
    "        self.test_acc = TM.Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _shared_step(self, batch):\n",
    "        features, true_labels = batch['image'], batch['label']\n",
    "        logits = self(features)\n",
    "        loss = F.cross_entropy(logits, true_labels)\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        return loss, true_labels, predicted_labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.train_acc(predicted_labels, true_labels)\n",
    "        self.log(\n",
    "            \"train_acc\", self.train_acc, prog_bar=True, on_epoch=True, on_step=False\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.val_acc(predicted_labels, true_labels)\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "        self.test_acc(predicted_labels, true_labels)\n",
    "        self.log(\"test_acc\", self.test_acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=self.cosine_t_max)\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": {\"scheduler\": sch, \"monitor\": \"train_loss\",\n",
    "                                                   \"interval\": \"step\", \"frequency\": 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
